{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "existingNodes = []\n",
    "rule_output = []\n",
    "links = []\n",
    "logical_rule = \"(\"  #Else it is arithmetic rule\n",
    "negated = \"~\"\n",
    "\n",
    "def output_data():\n",
    "    with open('node.json', 'w') as df:\n",
    "        json.dump(rule_output, df, indent = 4)\n",
    "\n",
    "    with open('link.json', 'w') as df:\n",
    "        json.dump(links,df, indent = 4)\n",
    "\n",
    "\n",
    "        \n",
    "unobservedAtoms = []\n",
    "def find_unobserved_atoms():\n",
    "    with open('data/knows_targets.txt', 'r') as target:\n",
    "        for line in target:\n",
    "            constants = line.strip().split(\"\\t\")\n",
    "            unobserved = 'KNOWS(\\''+constants[0]+'\\', \\''+constants[1]+'\\')'\n",
    "            print(unobserved)\n",
    "            unobservedAtoms.append(unobserved)\n",
    "        \n",
    "        \n",
    "#Finding predicates to split into groups for coloring\n",
    "def findPredicates():\n",
    "    with open('simple-acquaintances.data', 'r') as data:\n",
    "        predicate = {}\n",
    "        data = data.read().strip()\n",
    "        data = data.split(\"\\n\\n\")\n",
    "\n",
    "        predicates = data[0].split('\\n')[1:]\n",
    "        i = 1;\n",
    "        for pred in predicates:\n",
    "          \n",
    "            open_closed = pred.split(':')[1].strip()  #Finding open / closed predicate\n",
    "            pred = pred.strip().split('/')  #Finding predicate name\n",
    "            predicate[pred[0].upper()] = [i, open_closed]\n",
    "            i+=1\n",
    "                    \n",
    "    return predicate    \n",
    "\n",
    "\n",
    "\n",
    "#Finding links and atoms for logical rule\n",
    "def findLogicalRules(rules, Group):\n",
    "    groundAtoms = []  #List containing the ground atoms\n",
    "    rules = rules[1:-1] #Getting rid of the first and last parenthesis\n",
    "    rules = rules.strip()\n",
    "    groundRules = rules.split(\" | \")\n",
    "    \n",
    "    \n",
    "    \n",
    "    num_unobserved = 0\n",
    " #Grabbing each ground atom\n",
    "    for atoms in groundRules:\n",
    "     \n",
    "        # If ground atom has negation in front, parse out and just grab the ground atom within\n",
    "        if atoms[0] == negated:\n",
    "            groundAtom = atoms[3:-2]  # Obtain the ground atom for negated atoms\n",
    "        else:\n",
    "            groundAtom = atoms\n",
    "\n",
    "        predicate_group = groundAtom.split(\"(\")[0]  #Getting the predicate and making it \"case insensitive\"\n",
    "\n",
    "        if groundAtom not in existingNodes:\n",
    "            atom_data = {'groundAtoms': groundAtom, 'group': Group[predicate_group][0], 'type': Group[predicate_group][1], 'test': [a,b,c,d,e]}\n",
    "            # Adding oberved / unobserved parameter\n",
    "            if groundAtom in unobservedAtoms:\n",
    "                atom_data['unobserved'] = True\n",
    "                num_unobserved +=1  # Keeping track of number of unobserved atoms in the rule\n",
    "            else:\n",
    "                atom_data['unobserved'] = False\n",
    "            \n",
    "            rule_output.append(atom_data)\n",
    "            \n",
    "            existingNodes.append(groundAtom)\n",
    "        \n",
    "        groundAtoms.append(groundAtom)\n",
    "   \n",
    "\n",
    "    if num_unobserved > 1 :\n",
    "        links.append({'source': groundAtoms[0], 'target': groundAtoms[1], 'rule': rules})\n",
    "        links.append({'source': groundAtoms[0], 'target': groundAtoms[2], 'rule': rules}) \n",
    "        links.append({'source': groundAtoms[1], 'target': groundAtoms[2], 'rule': rules})\n",
    "        \n",
    "    else:\n",
    "        if groundAtoms[0] in unobservedAtoms:\n",
    "            links.append({'source': groundAtoms[0], 'target': groundAtoms[1], 'rule': rules})\n",
    "            links.append({'source': groundAtoms[0], 'target': groundAtoms[2], 'rule': rules}) \n",
    "        elif groundAtoms[1] in unobservedAtoms:\n",
    "            links.append({'source': groundAtoms[1], 'target': groundAtoms[0], 'rule': rules})\n",
    "            links.append({'source': groundAtoms[1], 'target': groundAtoms[2], 'rule': rules}) \n",
    "        else:\n",
    "            links.append({'source': groundAtoms[2], 'target': groundAtoms[1], 'rule': rules})\n",
    "            links.append({'source': groundAtoms[2], 'target': groundAtoms[0], 'rule': rules}) \n",
    "                                                                  \n",
    "                                                                  \n",
    "\n",
    "\n",
    "with open('data/sat.txt', 'r') as data:\n",
    "\n",
    "    predicateGroup = findPredicates()       #Getting predicate node group \n",
    "    for line in data:\n",
    "        line = line.strip()\n",
    "        data_split = line.split(\"\\t\")\n",
    "        \n",
    "#         print(data_split)\n",
    "        rules = data_split[2]\n",
    "        \n",
    "        #Just for logical rule links\n",
    "        if rules[0] == logical_rule:\n",
    "            findLogicalRules(rules, predicateGroup)\n",
    "            \n",
    "\n",
    "output_data()\n",
    "            \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNOWS('Elena', 'Steve')\n",
      "KNOWS('Elena', 'Jay')\n",
      "KNOWS('Elena', 'Ben')\n",
      "KNOWS('Elena', 'Alex')\n",
      "KNOWS('Elena', 'Arti')\n",
      "KNOWS('Elena', 'Dhanya')\n",
      "KNOWS('Elena', 'Sabina')\n",
      "KNOWS('Steve', 'Elena')\n",
      "KNOWS('Steve', 'Jay')\n",
      "KNOWS('Steve', 'Ben')\n",
      "KNOWS('Steve', 'Alex')\n",
      "KNOWS('Steve', 'Arti')\n",
      "KNOWS('Steve', 'Dhanya')\n",
      "KNOWS('Steve', 'Sabina')\n",
      "KNOWS('Jay', 'Elena')\n",
      "KNOWS('Jay', 'Steve')\n",
      "KNOWS('Jay', 'Ben')\n",
      "KNOWS('Jay', 'Alex')\n",
      "KNOWS('Jay', 'Arti')\n",
      "KNOWS('Jay', 'Dhanya')\n",
      "KNOWS('Jay', 'Sabina')\n",
      "KNOWS('Ben', 'Steve')\n",
      "KNOWS('Ben', 'Jay')\n",
      "KNOWS('Ben', 'Alex')\n",
      "KNOWS('Ben', 'Arti')\n",
      "KNOWS('Ben', 'Sabina')\n",
      "KNOWS('Alex', 'Elena')\n",
      "KNOWS('Alex', 'Steve')\n",
      "KNOWS('Alex', 'Jay')\n",
      "KNOWS('Alex', 'Ben')\n",
      "KNOWS('Alex', 'Arti')\n",
      "KNOWS('Alex', 'Dhanya')\n",
      "KNOWS('Alex', 'Sabina')\n",
      "KNOWS('Arti', 'Elena')\n",
      "KNOWS('Arti', 'Steve')\n",
      "KNOWS('Arti', 'Jay')\n",
      "KNOWS('Arti', 'Ben')\n",
      "KNOWS('Arti', 'Dhanya')\n",
      "KNOWS('Arti', 'Sabina')\n",
      "KNOWS('Dhanya', 'Elena')\n",
      "KNOWS('Dhanya', 'Steve')\n",
      "KNOWS('Dhanya', 'Jay')\n",
      "KNOWS('Dhanya', 'Ben')\n",
      "KNOWS('Dhanya', 'Alex')\n",
      "KNOWS('Dhanya', 'Arti')\n",
      "KNOWS('Dhanya', 'Sabina')\n",
      "KNOWS('Sabina', 'Elena')\n",
      "KNOWS('Sabina', 'Steve')\n",
      "KNOWS('Sabina', 'Jay')\n",
      "KNOWS('Sabina', 'Ben')\n",
      "KNOWS('Sabina', 'Alex')\n",
      "KNOWS('Sabina', 'Arti')\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
